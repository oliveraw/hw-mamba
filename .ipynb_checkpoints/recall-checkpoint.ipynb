{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are giving you a 2 layer transformer model in induction task. Check the data generation mechanism for the induction head and adjust it accordingly to your needs. For AR, you need to code one from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InductionAR(Dataset):\n",
    "    # In induction head we have ngram = 1. But the code provided is for general ngram setting. While using this, initialize ngram = 1. \n",
    "    \"\"\" Naive associative recall dataset \"\"\"\n",
    "    def __init__(self, num_examples, tokenizer, n_gram=1, n_ctx = 1024, seed = 0, train_split=0.8):\n",
    "        self.num_examples = num_examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_ctx = n_ctx\n",
    "        self.seed = seed\n",
    "        self.n_gram = n_gram\n",
    "        x, y = self.data_gen()\n",
    "        if train_split:\n",
    "            self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y, train_split)\n",
    "            self.train = self.numpy_to_tensor_dataset(self.train_x, self.train_y)\n",
    "            self.test = self.numpy_to_tensor_dataset(self.test_x, self.test_y)\n",
    "        else:\n",
    "            self.train_x, self.train_y, self.test_x, self.test_y = x, y, None, None\n",
    "            self.train = self.numpy_to_tensor_dataset(self.train_x, self.train_y)\n",
    "            self.test = None\n",
    "    def get_str_dataset(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            x_str = [self.tokenizer.decode(xi) for xi in self.train_x]\n",
    "            y_str = [self.tokenizer.decode([yi]) for yi in self.train_y]\n",
    "        elif split == \"test\":\n",
    "            x_str = [self.tokenizer.decode(xi) for xi in self.test_x]\n",
    "            y_str = [self.tokenizer.decode([yi]) for yi in self.test_y]\n",
    "        else:\n",
    "            raise ValueError(\"split should be either 'train' or 'test'\")\n",
    "        return x_str, y_str\n",
    "    def numpy_to_tensor_dataset(self, x, y):\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return TensorDataset(x, y)\n",
    "    def gen_single_example(self):\n",
    "        # get the vocab size\n",
    "        def count(str_x, str_n_gram_head):\n",
    "            counts = sum([\n",
    "                str_x.startswith(str_n_gram_head, i) for i in range(len(str_x))\n",
    "            ])\n",
    "            return counts\n",
    "        def gen_x():\n",
    "            gen_x_success = False\n",
    "            while not gen_x_success:\n",
    "                x = np.random.choice(vocab, self.n_ctx-self.n_gram*2, replace=True).tolist()\n",
    "                # remove the case where the n_gram_head is repeated in the sequence\n",
    "                for _ in range(10):\n",
    "                    pos = [i for i in range(len(x)-len(n_gram_head)+1) if x[i:i+len(n_gram_head)] == n_gram_head]\n",
    "                    if len(pos) == 0:\n",
    "                        gen_x_success = True\n",
    "                        break\n",
    "                    else:\n",
    "                        # remove the n_gram_head from x\n",
    "                        # get all positions of the n_gram_head in x\n",
    "                        for p in reversed(pos):\n",
    "                            # remove len(n_gram_head) elements from x starting from p\n",
    "                            x = x[:p] + x[p+len(n_gram_head):]\n",
    "                        # fill the rest of the sequence with random elements\n",
    "                        x.extend(np.random.choice(vocab, self.n_ctx-self.n_gram*2-len(x), replace=True).tolist())\n",
    "                x_test = \" \".join([str(xi) for xi in x])\n",
    "                if count(x_test, str_n_gram_head) == 0:\n",
    "                    gen_x_success = True\n",
    "\n",
    "            x_test = x + n_gram_head\n",
    "            # check if there's only one n_gram_head in the sequence\n",
    "            # to avoid the case where the n_gram_head has \n",
    "            # repeated structure such as x= [1, 2, 3, 1] , n_gram_head = [1, 1]\n",
    "            str_x_test = \" \"+\" \".join([str(xi) for xi in x_test])+ \" \"\n",
    "            if count(str_x_test, str_n_gram_head) > 1:\n",
    "                print(\"Error in gen_x\")\n",
    "                print(f\"str_x_test: {str_x_test}\", f\"str_n_gram_head: {str_n_gram_head}\", \n",
    "                      \"count: \", count(str_x_test, str_n_gram_head))\n",
    "            if count(str_x_test, str_n_gram_head) == 1:\n",
    "                return x\n",
    "            else:\n",
    "                return None\n",
    "        def insert_n_gram_head(x):\n",
    "            pos = random.randint(0, len(x)-1)\n",
    "            y = x[pos]\n",
    "            x_new = x[:pos] + n_gram_head + x[pos:] + n_gram_head\n",
    "            str_x_new = \" \"+\" \".join([str(xi) for xi in x_new])+\" \"\n",
    "\n",
    "            if count(str_x_new, str_n_gram_head) == 2:\n",
    "                return x_new, y\n",
    "            else:\n",
    "                return None, None\n",
    "        vocab_size = len(self.tokenizer)\n",
    "        vocab = list(range(vocab_size))\n",
    "        # set a deterministic n_gram_head\n",
    "        n_gram_head = list(range(self.n_gram))\n",
    "       \n",
    "        str_n_gram_head = \" \"+\" \".join([str(xi) for xi in n_gram_head])+\" \"\n",
    "        assert self.n_gram*2 < self.n_ctx, \"n_gram*2 should be less than n_ctx\"\n",
    "        success = False\n",
    "        while not success:\n",
    "            x = gen_x()\n",
    "            if x is not None:\n",
    "                for _ in range(10):\n",
    "                    x_new, y = insert_n_gram_head(x)\n",
    "                    if x_new is not None:\n",
    "                        success = True\n",
    "                        break\n",
    "        return x_new, y\n",
    "            \n",
    "    def data_gen(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        # get previous random status and recover after generating the dataset\n",
    "        random_status = random.getstate()\n",
    "        random.seed(self.seed)\n",
    "        for i in range(self.num_examples):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Generating example {i}\")\n",
    "            xi, yi = self.gen_single_example()\n",
    "            x.append(xi)\n",
    "            y.append(yi)\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        random.setstate(random_status)\n",
    "        return x, y\n",
    "    def split(self, x, y, train_ratio = 0.8):\n",
    "        num_train = int(len(x)*train_ratio)\n",
    "        train_x = x[:num_train]\n",
    "        train_y = y[:num_train]\n",
    "        test_x = x[num_train:]\n",
    "        test_y = y[num_train:]\n",
    "        return train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Random_tokenizer:\n",
    "    def __init__(self, vocab=None, vocab_size = None) -> None:\n",
    "        \"\"\" The init function of the tokenizer class.\n",
    "         one of vocab or vocab_size should be provided.\n",
    "         If vocab is provided, vocab_size will be ignored.\n",
    "         If vocab is not provided, vocab_size should be provided. we will generate a random vocab of vocab_size.\"\"\"\n",
    "        if vocab is not None:\n",
    "            self.vocab = vocab\n",
    "            self.vocab_size = len(vocab)\n",
    "        elif vocab_size is not None:\n",
    "            self.vocab_size = vocab_size\n",
    "            self.vocab = [str(i) for i in range(vocab_size)]\n",
    "        else:\n",
    "            raise ValueError(\"one of vocab or vocab_size should be provided.\")\n",
    "        self.vocab_dict = {v: i for i, v in enumerate(self.vocab)}\n",
    "        self.vocab_dict_inv = {i: v for i, v in enumerate(self.vocab)}\n",
    "    def encode(self, x):\n",
    "        \"\"\" Encode a string into a list of integers \"\"\"\n",
    "        return [self.vocab_dict[i] for i in x]\n",
    "    def decode(self, x):\n",
    "        \"\"\" Decode a list of integers into a string \"\"\"\n",
    "        return ' '.join([self.vocab_dict_inv[i] for i in x])\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab[i]\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab)\n",
    "    def __contains__(self, x):\n",
    "        return x in self.vocab\n",
    "    def __repr__(self):\n",
    "        return f\"Random_tokenizer(vocab_size={self.vocab_size})\"\n",
    "    def __str__(self):\n",
    "        return f\"Random_tokenizer(vocab_size={self.vocab_size})\"\n",
    "    def __call__(self, x):\n",
    "        return self.encode(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=11):\n",
    "        super(Block, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.c_attn = nn.Linear(embed_dim, embed_dim*3)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(max_len, max_len)))\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "    \n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, \n",
    "                 is_pe = False, max_len=11, \n",
    "                 attn_layers=2, block=None,\n",
    "                 **kwargs):\n",
    "        super(BaseNet, self).__init__()\n",
    "        if block is None:\n",
    "            raise ValueError(\"block type should be provided.\")\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.is_pe = is_pe\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pe = nn.Embedding(max_len, embed_dim) if is_pe else None\n",
    "        self.att = nn.ModuleList([block(embed_dim, max_len, **kwargs) for _ in range(attn_layers)])\n",
    "        self.ln = nn.ModuleList([LayerNorm(embed_dim, True) for _ in range(attn_layers)])\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "        print(f\"BaseNet with {attn_layers} layers of {block} blocks\")\n",
    "        print(f\"Embedding dimension: {embed_dim}\")\n",
    "        print(f\"Positional Encoding: {is_pe}\")\n",
    "        print(f\"Vocabulary size: {vocab_size}\")\n",
    "        print(f\"Context length: {max_len}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        if self.is_pe:\n",
    "            pos = torch.arange(0, t, dtype=torch.long, device=x.device)\n",
    "            pe_emb = self.pe(pos) if self.is_pe else 0\n",
    "            x = x + pe_emb\n",
    "        for layer, ln in zip(self.att, self.ln):\n",
    "            x = ln(layer(x))\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Generating example 0\n",
      "Generating example 1000\n",
      "Generating example 2000\n",
      "Generating example 3000\n",
      "Generating example 4000\n",
      "Generating example 5000\n",
      "Generating example 6000\n",
      "Generating example 7000\n",
      "Generating example 8000\n",
      "Generating example 9000\n",
      "Generating example 10000\n",
      "Generating example 11000\n",
      "Generating example 12000\n",
      "Generating example 13000\n",
      "Generating example 14000\n",
      "Generating example 15000\n",
      "Generating example 16000\n",
      "Generating example 17000\n",
      "Generating example 18000\n",
      "Generating example 19000\n",
      "Generating example 20000\n",
      "Generating example 21000\n",
      "Generating example 22000\n",
      "Generating example 23000\n",
      "Generating example 24000\n",
      "Generating example 25000\n",
      "Generating example 26000\n",
      "Generating example 27000\n",
      "Generating example 28000\n",
      "Generating example 29000\n",
      "Generating example 30000\n",
      "Generating example 31000\n",
      "Generating example 32000\n",
      "Generating example 33000\n",
      "Generating example 34000\n",
      "Generating example 35000\n",
      "Generating example 36000\n",
      "Generating example 37000\n",
      "Generating example 38000\n",
      "Generating example 39000\n",
      "Generating example 40000\n",
      "Generating example 41000\n",
      "Generating example 42000\n",
      "Generating example 43000\n",
      "Generating example 44000\n",
      "Generating example 45000\n",
      "Generating example 46000\n",
      "Generating example 47000\n",
      "Generating example 48000\n",
      "Generating example 49000\n",
      "Generating example 50000\n",
      "Generating example 51000\n",
      "Generating example 52000\n",
      "Generating example 53000\n",
      "Generating example 54000\n",
      "Generating example 55000\n",
      "Generating example 56000\n",
      "Generating example 57000\n",
      "Generating example 58000\n",
      "Generating example 59000\n",
      "Generating example 60000\n",
      "Generating example 61000\n",
      "Generating example 62000\n",
      "Generating example 63000\n",
      "Generating example 64000\n",
      "Generating example 65000\n",
      "Generating example 66000\n",
      "Generating example 67000\n",
      "Generating example 68000\n",
      "Generating example 69000\n",
      "Generating example 70000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m is_pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# the default positional embedding we are using is the learned positional embedding\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Random_tokenizer(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size)\n\u001b[0;32m---> 14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m InductionAR(num_examples, tokenizer, \u001b[38;5;241m1\u001b[39m, n_ctx\u001b[38;5;241m=\u001b[39mn_ctx, seed\u001b[38;5;241m=\u001b[39mseed, train_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     15\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m.\u001b[39mtrain, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m.\u001b[39mtest, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mInductionAR.__init__\u001b[0;34m(self, num_examples, tokenizer, n_gram, n_ctx, seed, train_split)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gram \u001b[38;5;241m=\u001b[39m n_gram\n\u001b[0;32m---> 10\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_gen()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_split:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit(x, y, train_split)\n",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36mInductionAR.data_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m xi, yi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_single_example()\n\u001b[1;32m    113\u001b[0m x\u001b[38;5;241m.\u001b[39mappend(xi)\n\u001b[1;32m    114\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(yi)\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mInductionAR.gen_single_example\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m gen_x()\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mInductionAR.gen_single_example.<locals>.gen_x\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# fill the rest of the sequence with random elements\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         x\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(vocab, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gram\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(x), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 58\u001b[0m x_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(xi) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count(x_test, str_n_gram_head) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     gen_x_success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# fill the rest of the sequence with random elements\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         x\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(vocab, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gram\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(x), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 58\u001b[0m x_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(xi) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count(x_test, str_n_gram_head) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     gen_x_success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "seed = 0\n",
    "n_ctx = 64 # training sequence length\n",
    "num_examples = 100000 # generate 100000 examples\n",
    "batch_size = 64 # batch size\n",
    "vocab_size = 16 # vocabulary size\n",
    "num_epochs = 250    # number of epochs\n",
    "attn_layers = 2 # number of attention layers\n",
    "embed_dim = 8 # embedding dimension\n",
    "is_pe = True  # the default positional embedding we are using is the learned positional embedding\n",
    "\n",
    "tokenizer = Random_tokenizer(vocab_size=vocab_size)\n",
    "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.8)\n",
    "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "attn_model = BaseNet(len(tokenizer), embed_dim, is_pe,  max_len=n_ctx*4, attn_layers=attn_layers, block=Block).to(device)\n",
    "    \n",
    "models = [attn_model] # add more models here when you have more models\n",
    "for model in models:\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)[:,-1]\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"epoch {epoch} loss: {total_loss/len(train_loader)}\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)[:,-1]\n",
    "            y_pred = F.softmax(y_pred, dim=-1)\n",
    "            y_pred = torch.argmax(y_pred, dim=-1)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"Test accuracy: {correct/total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data with length 32\n",
    "# test the model with the test data\n",
    "num_examples = 20000\n",
    "n_ctx = 32\n",
    "\n",
    "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
    "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "attn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        y_pred = attn_model(x)[:,-1]\n",
    "        y_pred = F.softmax(y_pred, dim=-1)\n",
    "        y_pred = torch.argmax(y_pred, dim=-1)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    print(f\"Test accuracy: {correct/total}\")\n",
    "\n",
    "# generate test data with length 16\n",
    "# test the model with the test data\n",
    "num_examples = 20000\n",
    "n_ctx = 16\n",
    "\n",
    "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
    "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "attn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        y_pred = attn_model(x)[:,-1]\n",
    "        y_pred = F.softmax(y_pred, dim=-1)\n",
    "        y_pred = torch.argmax(y_pred, dim=-1)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    print(f\"Test accuracy: {correct/total}\")\n",
    "\n",
    "# generate test data with length 128\n",
    "# test the model with the test data\n",
    "num_examples = 20000\n",
    "n_ctx = 128\n",
    "\n",
    "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
    "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "attn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        y_pred = attn_model(x)[:,-1]\n",
    "        y_pred = F.softmax(y_pred, dim=-1)\n",
    "        y_pred = torch.argmax(y_pred, dim=-1)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    print(f\"Test accuracy: {correct/total}\")\n",
    "\n",
    "\n",
    "# generate test data with length 256\n",
    "# test the model with the test data\n",
    "num_examples = 20000\n",
    "n_ctx = 256\n",
    "\n",
    "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
    "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "attn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        y_pred = attn_model(x)[:,-1]\n",
    "        y_pred = F.softmax(y_pred, dim=-1)\n",
    "        y_pred = torch.argmax(y_pred, dim=-1)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    print(f\"Test accuracy: {correct/total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
